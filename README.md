# NanoGPT

A character-level Generative Pre-trained Transformer (GPT) built from scratch in PyTorch. 
This model is trained on the complete works of William Shakespeare to generate original, authentic-sounding plays and poetry.
This project implements the core architecture of the "Transformer" paper (*Attention Is All You Need*) to learn the statistical structure of Shakespearean English. 

## Model Architecture
The model is a **Decoder-only Transformer** featuring:
* **Multi-Head Self-Attention:** Parallel processing of context to capture relationships between distant characters.
* **Residual Connections:** Deep network training stability (6+ layers).
* **Layer Normalization:** Pre-norm formulation for better convergence.
* **Feed-Forward Networks:** Position-wise processing with ReLU activation.

**Hyperparameters:**
* Context Window: 256 characters
* Embedding Dimension: 384
* Layers: 6
* Attention Heads: 6
* Parameters: ~10 Million

## ðŸ“‚ Project Structure
The code is modularized for clarity and scalability:

```text
Shakespeare-GPT/
â”œâ”€â”€ data/
â”‚   â””â”€â”€ input.txt          # Training corpus (Complete Shakespeare)
â”œâ”€â”€ modules/
â”‚   â”œâ”€â”€ attention_head.py  # Head & MultiHeadAttention classes
â”‚   â”œâ”€â”€ feedforward.py     # FeedForward network 
â”‚   â””â”€â”€ residual_block.py  # Transformer Block (Attention + FFN + Residuals)
â”œâ”€â”€ bigram.py              # Main training script & Model assembly
â””â”€â”€ README.md
